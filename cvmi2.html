<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Xiaojuan Qi, University of Hong Kong"> 
<meta name="description" content="Xiaojuan Qi's home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiaojuan Qi</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<script>
    const labMembers = [
        { name: 'Xiaojuan Qi', position: 'Principal Investigator', img: 'pics/xiaojuan.jpg' },
	{ name: 'Xiaojuan Qi', position: 'Principal Investigator', img: 'pics/xiaojuan.jpg' },
	{ name: 'Xiaojuan Qi', position: 'Principal Investigator', img: 'pics/xiaojuan.jpg' },
	{ name: 'Xiaojuan Qi', position: 'Principal Investigator', img: 'pics/xiaojuan.jpg' },
        // Add the other lab members with their names, positions, and image file names
    ];

    function generateLabMembersHTML() {
        let labMembersHTML = '<div class="lab-members"><ul>';
        labMembers.forEach((member, i) => {
            labMembersHTML += `
            <li>
                <div class="member-photo">
                    <img src="${member.img}" alt="Lab member ${member.name}" />
                </div>
                <div class="member-info">
                    <h3>${member.name}</h3>
                    <p>${member.position}</p>
                </div>
            </li>`;
        });
        labMembersHTML += '</ul></div>';
        return labMembersHTML;
    }

    window.addEventListener('DOMContentLoaded', () => {
        const labMembersSection = document.getElementById('lab-members-section');
        labMembersSection.innerHTML = generateLabMembersHTML();
    });
</script>

<style>
.lab-members ul {
  display: flex;
  flex-wrap: wrap;
  list-style: none;
  padding: 0;
  margin: 0;
}

.lab-members li {
  flex: 1;
  min-width: calc(100% / 4 - 30px);
  padding: 15px;
  box-sizing: border-box;
  display: flex;
  flex-direction: column;
  align-items: center;
  margin-bottom: 30px;
}

.member-photo img {
  width: 100%;
  height: auto;
  border-radius: 5px;
}

.member-info {
  margin-top: 10px;
  text-align: center;
}

@media (max-width: 1024px) {
  .lab-members li {
    min-width: calc(100% / 3 - 30px);
  }
}

@media (max-width: 768px) {
  .lab-members li {
    min-width: calc(50% - 30px);
  }
}

@media (max-width: 480px) {
  .lab-members li {
    min-width: 100%;
  }
}
</style>

</head>
<body >

<div id="layout-content" style="margin-top:25px">



<h2>Computer Vision and Machine Intelligence Lab (CVMI Lab)</h2>
<p>
 We're a group of people dedicated to advancing computer vision, deep learning, and artificial intelligence. Our focus is to push visual intelligence forward towards open-world scenarios. We believe it has the potential to transform future manufacturing, robotics, autonomous driving, and virtual reality, and thereby generate positive impacts on our lives. Currently, we are interested in exploring the following topics: 
	<li>
	Develop 3D reconstruction and generation techniques for digitizing real world and constructing visual data simuators.  
	</li>
	<li>
	Develop the inference between large vision and language models for open-vocabulary understanding and intelligent reasoning.
	</li>
	<li>
	Develop reliable and trustworthy visual models with lifelong learning abilities .
	</li>
	<li>
	Develop efficient training and inference methods for "green", sustainable and cost effective visual intelligence.  
	</li>
</p>

<h2>Lab Updates</h2>
<ul>
<li>
Dr. Xiaojuan Qi will be an area chair for NeurIPS 2023.
</li>
<li>
Seven papers are accepted to CVPR Congratulations to my students and collaborators.
</li>
<li>
Three papers are accepted to ICLR 2023 with two spotlight papers. Congratulations to my students and collaborators.
</li>
<li>
Six papers are accepted to NeurIPS 2022. Congratulations to my students and collaborators. 
</li>
</ul>
<h2>Lab Members</h2>
<div id="lab-members-section"></div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88615920-1', 'auto');
  ga('send', 'pageview');

</script>
</div>
</body>
</html>
