<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Xiaojuan Qi, University of Hong Kong"> 
<meta name="description" content="Xiaojuan Qi's home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiaojuan Qi</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body >

<div id="layout-content" style="margin-top:25px">



<h2>Computer Vision and Machine Intelligence Lab (CVMI Lab)</h2>
<p>
 we're a group of people, dedicated to advancing the fields of computer vision, deep learning, and artificial intelligence. Our aim is to push visual intelligence forward towards open-world scenarios. We believe it has the potential to transform future manufacturing, robotics, autonomous driving, and virtual reality, and thereby generate positive impacts on our lives. Currently, we are interested in exploring the topics: 
	<li>
	Develop 3D reconstruction and generation (synthesis) techniques for digitizing real world and constructing visual data simuators.  
	</li>
	<li>
	Develop the inference between large vision and language models for open-vocabulary understanding and intelligent reasoning.
	</li>
	<li>
	Develop reliable and trustworthy visual models with lifelong learning abilities .
	</li>
	<li>
	Develop efficient training and inference methods for "green", sustainable and cost effective visual intelligence.  
	</li>
</p>

<h2>Lab Updates</h2>
<ul>
<li>
Dr. Xiaojuan Qi will be an area chair for NeurIPS 2023.
</li>
<li>
Seven papers are accepted to CVPR 2023. Congratulations to my students and collaborators.
</li>
<li>
Five papers are accepted to NeurIPS 2022. Congratulations to my students and collaborators. 
</li>
</ul>
	
<h2>Lab Members</h2>



<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88615920-1', 'auto');
  ga('send', 'pageview');

</script>

</div>
</body>
</html>
